# ğŸ§± Key Safety Categories Covered by WSO2 AI Guardrails

[![AI Gateway]({{base_path}}/assets/img/learn/ai-gateway/ai-guardrail-safety-categories.png){: style="width:40%"}]({{base_path}}/assets/img/learn/ai-gateway/ai-guardrail-safety-categories.png)

WSO2 AI Guardrails comprehensively address critical safety aspects across four foundational categories, ensuring secure, compliant, and reliable AI interactions:

| Category                  | Description                                                                                   |
|---------------------------|-----------------------------------------------------------------------------------------------|
| ğŸ” [**LLM Safety**](../safety-categories/llm-safety.md)           | Protects against prompt injection attacks and enforces proper prompt structures to maintain model integrity. |
| â˜£ï¸ [**Content Safety**](../safety-categories/content-safety.md)       | Detects and filters toxic, harmful, or offensive content to ensure safe and appropriate AI outputs.             |
| ğŸ› [**Content Usage Control**](../safety-categories/content-usage-control.md) | Implements organizational policies by enforcing word, sentences, and content usage guidelines consistently.    |
| ğŸ•µï¸ [**PII Safety**](../safety-categories/pii-safety.md)           | Identifies and redacts personally identifiable information (PII) to uphold privacy and regulatory compliance.    |


Explore the comprehensive guardrail capabilities offered within each safety category.
